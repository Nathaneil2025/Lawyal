      - name: EKS: annihilate nodegroups (any state) & kill nodes
        run: |
          set -e
          CLUSTER="$CLUSTER_NAME"
          REGION="$AWS_REGION"

          echo "🔎 Discovering nodegroups for $CLUSTER..."
          NODEGROUPS="$(aws eks list-nodegroups --cluster-name "$CLUSTER" --region "$REGION" --query 'nodegroups[]' --output text || true)"

          # Helper: get ASGs for a nodegroup via tagging API (most reliable)
          get_asgs_for_ng() {
            local ng="$1"
            aws resourcegroupstaggingapi get-resources \
              --region "$REGION" \
              --tag-filters "Key=eks:cluster-name,Values=$CLUSTER" "Key=eks:nodegroup-name,Values=$ng" \
              --resource-type-filters "autoscaling:autoScalingGroup" \
              --query 'ResourceTagMappingList[].ResourceARN' --output text \
            | sed 's#.*:autoScalingGroup/##g' || true
          }

          # Helper: terminate all instances in provided ASG names
          kill_asg_instances() {
            for A in "$@"; do
              [ -z "$A" ] && continue
              echo "🛑 Zeroing ASG $A"
              aws autoscaling update-auto-scaling-group \
                --auto-scaling-group-name "$A" \
                --min-size 0 --max-size 0 --desired-capacity 0 \
                --region "$REGION" || true

              IDS="$(aws autoscaling describe-auto-scaling-groups --auto-scaling-group-names "$A" --region "$REGION" \
                   --query 'AutoScalingGroups[].Instances[].InstanceId' --output text || true)"
              if [ -n "$IDS" ]; then
                echo "🪓 Terminating instances in $A: $IDS"
                aws ec2 terminate-instances --instance-ids $IDS --region "$REGION" || true
                aws ec2 wait instance-terminated --instance-ids $IDS --region "$REGION" || true
              fi
            done
          }

          # Helper: force delete ASGs and try to delete their LaunchTemplates
          delete_asgs_and_lts() {
            for A in "$@"; do
              [ -z "$A" ] && continue
              echo "🧨 Force deleting ASG $A"
              # Try to capture its LaunchTemplate before deletion
              LT_ID="$(aws autoscaling describe-auto-scaling-groups \
                 --auto-scaling-group-names "$A" --region "$REGION" \
                 --query 'AutoScalingGroups[0].LaunchTemplate.LaunchTemplateId' --output text 2>/dev/null || echo '')"
              aws autoscaling delete-auto-scaling-group --auto-scaling-group-name "$A" --force-delete --region "$REGION" || true
              # Best-effort LT deletion (may fail if still referenced; that's fine)
              if [ -n "$LT_ID" ] && [ "$LT_ID" != "None" ]; then
                echo "🧹 Deleting Launch Template $LT_ID (best-effort)"
                aws ec2 delete-launch-template --launch-template-id "$LT_ID" --region "$REGION" || true
              fi
            done
          }

          # 1) Handle all known nodegroups
          for NG in $NODEGROUPS; do
            echo "➡️  Working on nodegroup: $NG"

            # Try a clean scale-to-zero first (valid values: maxSize must be >=1)
            echo "↘️ Scaling $NG to 0 (min=0, desired=0, max=1)"
            aws eks update-nodegroup-config \
              --cluster-name "$CLUSTER" --nodegroup-name "$NG" --region "$REGION" \
              --scaling-config minSize=0,maxSize=1,desiredSize=0 || true

            # Grab ASGs via tagging (and, if missing, from EKS API)
            ASGS="$(get_asgs_for_ng "$NG")"
            if [ -z "$ASGS" ]; then
              ASGS="$(aws eks describe-nodegroup --cluster-name "$CLUSTER" --nodegroup-name "$NG" --region "$REGION" \
                      --query 'nodegroup.resources.autoScalingGroups[].name' --output text 2>/dev/null || true)"
            fi
            echo "🔗 Backing ASGs for $NG: ${ASGS:-<none>}"

            # Kill any instances, then try standard NG delete
            [ -n "$ASGS" ] && kill_asg_instances $ASGS

            echo "🧹 Deleting nodegroup $NG (standard delete)..."
            aws eks delete-nodegroup --cluster-name "$CLUSTER" --nodegroup-name "$NG" --region "$REGION" || true

            # Poll status; if DELETE_FAILED or still present, nuke ASGs and retry
            for i in {1..30}; do
              STATUS="$(aws eks describe-nodegroup --cluster-name "$CLUSTER" --nodegroup-name "$NG" --region "$REGION" \
                        --query 'nodegroup.status' --output text 2>/dev/null || echo 'MISSING')"
              echo "   → $NG status: $STATUS"
              if [ "$STATUS" = "MISSING" ]; then
                break
              fi
              if [ "$STATUS" = "DELETE_FAILED" ] || [ $i -eq 10 ]; then
                echo "⚠️  $NG not deleting cleanly, FORCE-removing backing ASGs..."
                [ -n "$ASGS" ] && delete_asgs_and_lts $ASGS
                # Retry delete
                aws eks delete-nodegroup --cluster-name "$CLUSTER" --nodegroup-name "$NG" --region "$REGION" || true
              fi
              sleep 15
            done
          done

          # 2) Kill any remaining EC2 nodes that belong to this cluster (catch-all)
          echo "🧹 Catch-all: terminating stray EC2 nodes for $CLUSTER..."
          CATCH_IDS="$(aws ec2 describe-instances --region "$REGION" \
            --filters "Name=tag:aws:eks:cluster-name,Values=$CLUSTER" \
                      "Name=instance-state-name,Values=pending,running,stopping,stopped" \
            --query 'Reservations[].Instances[].InstanceId' --output text || true)"
          # Also try common Kubernetes tag (some AMIs add it)
          if [ -z "$CATCH_IDS" ]; then
            CATCH_IDS="$(aws ec2 describe-instances --region "$REGION" \
              --filters "Name=tag:kubernetes.io/cluster/${CLUSTER},Values=owned,shared" \
                        "Name=instance-state-name,Values=pending,running,stopping,stopped" \
              --query 'Reservations[].Instances[].InstanceId' --output text || true)"
          fi
          if [ -n "$CATCH_IDS" ]; then
            echo "🪓 Terminating stray instances: $CATCH_IDS"
            aws ec2 terminate-instances --instance-ids $CATCH_IDS --region "$REGION" || true
            aws ec2 wait instance-terminated --instance-ids $CATCH_IDS --region "$REGION" || true
          else
            echo "✅ No stray instances found."
          fi

          # 3) As a last check, if any nodegroups still exist, try one final delete
          REMAIN="$(aws eks list-nodegroups --cluster-name "$CLUSTER" --region "$REGION" --query 'nodegroups[]' --output text || true)"
          if [ -n "$REMAIN" ]; then
            echo "♻️ Final delete attempt for remaining nodegroups: $REMAIN"
            for NG in $REMAIN; do
              aws eks delete-nodegroup --cluster-name "$CLUSTER" --nodegroup-name "$NG" --region "$REGION" || true
            done
            # brief poll
            for NG in $REMAIN; do
              for i in {1..20}; do
                STATUS="$(aws eks describe-nodegroup --cluster-name "$CLUSTER" --nodegroup-name "$NG" --region "$REGION" \
                          --query 'nodegroup.status' --output text 2>/dev/null || echo 'MISSING')"
                echo "   → $NG status: $STATUS"
                [ "$STATUS" = "MISSING" ] && break
                sleep 10
              done
            done
          fi

          echo "✅ Nodegroups and nodes removed (or detached) — safe to delete cluster next."
