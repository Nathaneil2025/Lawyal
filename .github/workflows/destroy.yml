name: Destroy Infrastructure

on:
  workflow_dispatch:

permissions:
  id-token: write
  contents: read

jobs:
  destroy:
    runs-on: ubuntu-latest
    env:
      AWS_REGION: eu-central-1
      CLUSTER_NAME: lawyal-eks
      ECR_REPOSITORY: lawyal/flask-app

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::518394500999:role/GitHubActionsRole
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      # 1) Clean K8s workloads to free cloud deps
      - name: Kube cleanup workloads
        run: |
          set -e
          aws eks update-kubeconfig --name "$CLUSTER_NAME" --region "$AWS_REGION" || true

          echo "âŽˆ Uninstalling Helm releases..."
          helm uninstall flask-app || true

          echo "ðŸ§¹ Deleting LoadBalancer Services..."
          kubectl get svc -A -o json 2>/dev/null \
          | jq -r '.items[] | select(.spec.type=="LoadBalancer") | [.metadata.namespace,.metadata.name] | @tsv' \
          | while IFS=$'\t' read -r ns name; do
              kubectl delete svc -n "$ns" "$name" --wait=false || true
            done

          echo "ðŸ§¹ Deleting PVCs..."
          kubectl get pvc -A --no-headers 2>/dev/null \
          | awk '{print $1,$2}' \
          | while read ns name; do
              kubectl delete pvc -n "$ns" "$name" --wait=false || true
            done

      # 2) Scale down nodegroups FIRST and suspend ASG launches so nothing respawns
      # 2) Scale down nodegroups FIRST and suspend ASG launches so nothing respawns
      - name: Scale down nodegroups & suspend ASG launches
        run: |
          set -e
          CLUSTER="$CLUSTER_NAME"
          REGION="$AWS_REGION"

          echo "ðŸ”Ž Fetching nodegroups..."
          NODEGROUPS=$(aws eks list-nodegroups --cluster-name "$CLUSTER" --region "$REGION" --query "nodegroups[]" --output text 2>/dev/null || true)

          for NG in $NODEGROUPS; do
            echo "â†˜ï¸ Scaling $NG to 0 (min=0, desired=0, max=0)..."
            aws eks update-nodegroup-config \
              --cluster-name "$CLUSTER" \
              --nodegroup-name "$NG" \
              --scaling-config minSize=0,maxSize=0,desiredSize=0 \
              --region "$REGION" || true

            echo "â¸ Suspending ASG launches for $NG..."
            ASGS=$(aws eks describe-nodegroup \
              --cluster-name "$CLUSTER" \
              --nodegroup-name "$NG" \
              --region "$REGION" \
              --query "nodegroup.resources.autoScalingGroups[].name" \
              --output text 2>/dev/null || true)

            for A in $ASGS; do
              aws autoscaling suspend-processes \
                --auto-scaling-group-name "$A" \
                --scaling-processes Launch ReplaceUnhealthy \
                --region "$REGION" || true

              echo "â³ Forcing ASG $A to 0 capacity..."
              aws autoscaling update-auto-scaling-group \
                --auto-scaling-group-name "$A" \
                --min-size 0 --max-size 0 --desired-capacity 0 \
                --region "$REGION" || true
            done
          done

          echo "ðŸ›‘ Immediately terminating any leftover EC2 instances..."
          IDS=$(aws ec2 describe-instances --region "$REGION" \
            --filters "Name=tag:aws:eks:cluster-name,Values=$CLUSTER" \
            --query "Reservations[].Instances[].InstanceId" \
            --output text || true)
          if [ -n "$IDS" ]; then
            aws ec2 terminate-instances --instance-ids $IDS --region "$REGION" || true
            aws ec2 wait instance-terminated --instance-ids $IDS --region "$REGION" || true
          fi


      # 3) Terminate any straggler EC2s (should be none after step 2)
      - name: Terminate EC2 nodes
        run: |
          echo "ðŸ›‘ Terminating EC2 worker nodes (if any left)..."
          IDS=$(aws ec2 describe-instances --region "$AWS_REGION" \
            --filters "Name=tag:aws:eks:cluster-name,Values=$CLUSTER_NAME" \
            --query "Reservations[].Instances[].InstanceId" --output text || true)
          if [ -n "$IDS" ]; then
            aws ec2 terminate-instances --instance-ids $IDS --region "$AWS_REGION" || true
            aws ec2 wait instance-terminated --instance-ids $IDS --region "$AWS_REGION" || true
          fi

      # 4) Delete nodegroups
      - name: Delete Nodegroups
        run: |
          echo "ðŸ§¹ Deleting EKS nodegroups..."
          NODEGROUPS=$(aws eks list-nodegroups --cluster-name "$CLUSTER_NAME" --region "$AWS_REGION" --query "nodegroups[]" --output text 2>/dev/null || true)
          for NG in $NODEGROUPS; do
            aws eks delete-nodegroup --cluster-name "$CLUSTER_NAME" --nodegroup-name "$NG" --region "$AWS_REGION" || true
            for i in {1..20}; do
              STATUS=$(aws eks describe-nodegroup --cluster-name "$CLUSTER_NAME" --nodegroup-name "$NG" --region "$AWS_REGION" --query "nodegroup.status" --output text 2>/dev/null || echo "MISSING")
              echo "   â†’ $NG: $STATUS"
              [ "$STATUS" = "MISSING" ] && break
              sleep 15
            done
          done

      # 5) Delete Cluster (skip cleanly if not found)
      - name: Delete EKS cluster
        run: |
          echo "ðŸ§¹ Deleting EKS cluster..."
          if aws eks describe-cluster --name "$CLUSTER_NAME" --region "$AWS_REGION" >/dev/null 2>&1; then
            aws eks delete-cluster --name "$CLUSTER_NAME" --region "$AWS_REGION" || true
            aws eks wait cluster-deleted --name "$CLUSTER_NAME" --region "$AWS_REGION" || true
          else
            echo "âš ï¸ Cluster $CLUSTER_NAME not found, skipping..."
          fi

      # 6) Delete Load Balancers (ALB/NLB/Classic)
      - name: Delete Load Balancers
        run: |
          echo "ðŸ§¹ Deleting ALB/NLBs..."
          LBS=$(aws elbv2 describe-load-balancers --region "$AWS_REGION" --query 'LoadBalancers[].LoadBalancerArn' --output text || true)
          for lb in $LBS; do
            aws elbv2 delete-load-balancer --load-balancer-arn "$lb" --region "$AWS_REGION" || true
          done

          echo "ðŸ§¹ Deleting classic ELBs..."
          CLB_NAMES=$(aws elb describe-load-balancers --region "$AWS_REGION" --query 'LoadBalancerDescriptions[].LoadBalancerName' --output text 2>/dev/null || true)
          for name in $CLB_NAMES; do
            aws elb delete-load-balancer --load-balancer-name "$name" --region "$AWS_REGION" || true
          done

      # 7) Delete Security Groups (non-default)
      - name: Delete Security Groups
        run: |
          echo "ðŸ§¹ Deleting Security Groups..."
          for sg in $(aws ec2 describe-security-groups --region "$AWS_REGION" \
            --query "SecurityGroups[?GroupName!='default'].GroupId" --output text || true); do
            aws ec2 delete-security-group --group-id "$sg" --region "$AWS_REGION" || true
          done

      # 8) Delete ENIs created by EKS/ELB
      - name: Delete ENIs
        run: |
          echo "ðŸ§¹ Cleaning orphaned ENIs..."
          ENIS=$(aws ec2 describe-network-interfaces --region "$AWS_REGION" \
            --filters "Name=description,Values=amazon-eks*,ELB * ,*k8s*" \
            --query 'NetworkInterfaces[].NetworkInterfaceId' --output text || true)
          for ENI in $ENIS; do
            ATTACH_ID=$(aws ec2 describe-network-interfaces --network-interface-ids "$ENI" --region "$AWS_REGION" \
              --query 'NetworkInterfaces[0].Attachment.AttachmentId' --output text 2>/dev/null || echo "")
            if [ "$ATTACH_ID" != "None" ] && [ -n "$ATTACH_ID" ]; then
              aws ec2 detach-network-interface --attachment-id "$ATTACH_ID" --region "$AWS_REGION" || true
              sleep 5
            fi
            aws ec2 delete-network-interface --network-interface-id "$ENI" --region "$AWS_REGION" || true
          done

      # 9) Purge ECR (optional)
      - name: Purge ECR repository
        run: |
          echo "ðŸ§¹ Purging ECR images..."
          aws ecr list-images --repository-name "$ECR_REPOSITORY" --region "$AWS_REGION" --query 'imageIds[*]' --output json \
          | jq -c '.[]' | while read img; do
              aws ecr batch-delete-image --repository-name "$ECR_REPOSITORY" --region "$AWS_REGION" --image-ids "$img" || true
            done
          aws ecr delete-repository --repository-name "$ECR_REPOSITORY" --region "$AWS_REGION" --force || true

      # 10) Terraform destroy (VPC & friends)
      - name: Terraform Init
        run: terraform init -reconfigure

      - name: Terraform Destroy
        run: terraform destroy -auto-approve -parallelism=1 || true
